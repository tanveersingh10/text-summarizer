# params for the deep learning algorithm

TrainingArguments:
  num_train_epochs: 10
  warmup_steps: 500  # use a very low learning rate for a set number of training steps, this has the benefit of slowly starting to tune things like attention mechanisms
  per_device_train_batch_size: 1
  weight_decay: 0.01 # prevent overfitting by penalizing large weights.
  logging_steps: 10 #logs metrics every 10 steps
  evaluation_strategy: steps #steps means evaluation will be performed at every 'eval_steps'.
  eval_steps: 500 
  save_steps: 1e6
  gradient_accumulation_steps: 16 # number of steps to accumulate gradients before performing an optimization step. Allows bigger batch sizes without using more memory 



